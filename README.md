# Image-Captioning-and-Emoticon-Analysis-using-CNN-and-LSTM

Image captioning is used to generate automatic captions that describe an image. Here we present a neural network-based image captioning implementation (i.e. CNN+ RNN). First, the model extracts CNN's image feature and then generates RNN captions. CNN is VGG16 and the LSTM standard for RNN. To predict the caption of images, Normal Sampling and Beam Search were used. 
Dataset used- Here we have used Flickr8k dataset. Flickr 8K has 6000 training images, 1000 validation images and 1000 testing images. Each image has 5 captions describing it.
Keras with Tensorflow backend has been used while the features are being extracted using InceptionV3. We used Beam search here with k=3, 5, 7 and an Argmax search to predict the image captions. Argmax Search is where the maximum index value (argmax) is extracted and appended to the result in the long predicted vector of 8256. This is done until we hit < end > or the caption maximum length. Beam Search is where we take predictions from top k, feed them in the model again, and then sort them using the model's returned probabilities. So, there will always be the top k predictions in the list. Ultimately, we take the one with the highest probability and go through it until we find < end > or reach the maximum length of the caption.
